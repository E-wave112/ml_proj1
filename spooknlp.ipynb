{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "spooknlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1X53c-k4pUwaOxY810QHkZmLT1A-JKaqQ",
      "authorship_tag": "ABX9TyPd8dVJ7CsMKOVMAVPaiTcg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/E-wave112/ml_proj1/blob/master/spooknlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUO3ueSYX1f2",
        "outputId": "2c8bbeb0-16e0-4d56-b895-f42c92af9e98"
      },
      "source": [
        "#spooky nlp\r\n",
        "##load datsets\r\n",
        "import pandas as pd\r\n",
        "from pandas import DataFrame\r\n",
        "import numpy as np\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/spook/train.csv')\r\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/spook/test.csv')\r\n",
        "submission_file = pd.read_csv('/content/drive/MyDrive/spook/sample_submission.csv')\r\n",
        "id = test_df['id']\r\n",
        "test_df.drop(test_df[['id']],1,inplace=True)\r\n",
        "train_df.drop(train_df[['id']],1,inplace=True)\r\n",
        "\r\n",
        "\r\n",
        "label_enc = LabelEncoder()\r\n",
        "\r\n",
        "train_df['author'].replace('EAP',1,inplace=True)\r\n",
        "train_df['author'].replace('HPL',2,inplace=True)\r\n",
        "train_df['author'].replace('MWS',3,inplace=True)\r\n",
        "\r\n",
        "print(train_df.shape)\r\n",
        "\r\n",
        "##tokeize our text\r\n",
        "\r\n",
        "count_vectorizer = CountVectorizer()\r\n",
        "\r\n",
        "train_df['text'] = count_vectorizer.fit_transform(train_df['text']).todense()\r\n",
        "test_df['text'] = count_vectorizer.transform(test_df['text']).todense()\r\n",
        "print(train_df[:10])\r\n",
        "\r\n",
        "log_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\r\n",
        "train_df = np.array(train_df).reshape(len(train_df),-1)\r\n",
        "train_df = pd.DataFrame(train_df,columns=['text','author'])\r\n",
        "print(type(train_df))\r\n",
        "# train_df['author'] = np.array(train_df['author']).reshape(len(train_df['author']),-1)\r\n",
        "log_clf.fit(train_df[['text']],train_df[['author']])\r\n",
        "\r\n",
        "test_df = np.array(test_df).reshape(len(test_df),-1)\r\n",
        "pred = log_clf.predict_proba(test_df)\r\n",
        "submit = pd.DataFrame({\r\n",
        "    'id':id,\r\n",
        "    'EAP':pred[:,0],\r\n",
        "    'HPL':pred[:,1],\r\n",
        "    'MWS':pred[:,2]\r\n",
        "})\r\n",
        "submit.to_csv('/content/drive/MyDrive/spook/submission2.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19579, 2)\n",
            "   text  author\n",
            "0     0       1\n",
            "1     0       2\n",
            "2     0       1\n",
            "3     0       3\n",
            "4     0       2\n",
            "5     0       3\n",
            "6     0       1\n",
            "7     0       1\n",
            "8     0       1\n",
            "9     0       3\n",
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MacoDdENBftP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIFuQN5zlLDy",
        "outputId": "d13000b0-fd44-4261-9fcc-3f8d3330cd15"
      },
      "source": [
        "###implementing dl and lstm on this model\r\n",
        "import re\r\n",
        "import pandas as pd\r\n",
        "from pandas import DataFrame\r\n",
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "plt.style.use('dark_background')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop=set(stopwords.words('english'))\r\n",
        "import string\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D\r\n",
        "\r\n",
        "\r\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/spook/train.csv')\r\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/spook/test.csv')\r\n",
        "submission_file = pd.read_csv('/content/drive/MyDrive/spook/sample_submission.csv')\r\n",
        "\r\n",
        "id = test_df['id']\r\n",
        "test_df.drop(test_df[['id']],1,inplace=True)\r\n",
        "train_df.drop(train_df[['id']],1,inplace=True)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train_df['author'].replace('EAP',1,inplace=True)\r\n",
        "train_df['author'].replace('HPL',2,inplace=True)\r\n",
        "train_df['author'].replace('MWS',3,inplace=True)\r\n",
        "\r\n",
        "def remove_nums(num):\r\n",
        "    return ''.join(i for i in num if not i.isdigit())\r\n",
        "\r\n",
        "train_df['text'] = train_df['text'].apply(remove_nums)\r\n",
        "test_df['text'] = test_df['text'].apply(remove_nums)\r\n",
        "\r\n",
        "\r\n",
        "def remove_stopwords(text):\r\n",
        "    words = [w for w in text if w not in stopwords.words('english')]\r\n",
        "    return words\r\n",
        "    \r\n",
        "# train_df['text'] = train_df['text'].apply(remove_stopwords)\r\n",
        "# test_df['text'] = test_df['text'].apply(remove_stopwords)\r\n",
        "# print(type(train_df['text']))\r\n",
        "replace_list = {r\"i'm\": 'i am',\r\n",
        "                r\"'re\": ' are',\r\n",
        "                r\"let’s\": 'let us',\r\n",
        "                r\"'s\":  ' is',\r\n",
        "                r\"'ve\": ' have',\r\n",
        "                r\"can't\": 'can not',\r\n",
        "                r\"cannot\": 'can not',\r\n",
        "                r\"shan’t\": 'shall not',\r\n",
        "                r\"n't\": ' not',\r\n",
        "                r\"'d\": ' would',\r\n",
        "                r\"'ll\": ' will',\r\n",
        "                r\"'scuse\": 'excuse',\r\n",
        "                ',': ' ,',\r\n",
        "                '.': ' .',\r\n",
        "                '!': ' !',\r\n",
        "                '?': ' ?',\r\n",
        "                '\\s+': ' '}\r\n",
        "# def clean_text(text):\r\n",
        "#     text = text.lower()\r\n",
        "#     for s in replace_list:\r\n",
        "#         text = text.replace(s, replace_list[s])\r\n",
        "#     text = ' '.join(text.split())\r\n",
        "#     return text\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# train_df['text'] = train_df['text'].apply(lambda x:clean_text(x))\r\n",
        "# test_df['text'] = test_df['text'].apply(lambda x:clean_text(x))\r\n",
        "\r\n",
        "\r\n",
        "max_words = 8192\r\n",
        "tokenizer = Tokenizer(\r\n",
        "    num_words = max_words,\r\n",
        "    filters = '\"#$%&()*+-/:;<=>@[\\]^_`{|}~',\r\n",
        "    lower=True\r\n",
        ")\r\n",
        "\r\n",
        "# phrase_len = train_df['text'].apply(lambda p:len(p.split(' ')))\r\n",
        "\r\n",
        "\r\n",
        "# max_phrase_len = phrase_len.max()\r\n",
        "# print(max_phrase_len)\r\n",
        "tokenizer.fit_on_texts(train_df['text'])\r\n",
        "tokenizer.fit_on_texts(test_df['text'])\r\n",
        "\r\n",
        "X = tokenizer.texts_to_sequences(train_df['text'])\r\n",
        "X_test = tokenizer.texts_to_sequences(test_df['text'])\r\n",
        "\r\n",
        "\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "\r\n",
        "#print(vocab_size)\r\n",
        "X  = pad_sequences(X,maxlen=100)\r\n",
        "X_test = pad_sequences(X_test,maxlen=100)\r\n",
        "Y = np.array(train_df['author'])\r\n",
        "Y =to_categorical(Y)\r\n",
        "Y = Y[:,[1,2,3]]\r\n",
        "print(Y[:5])\r\n",
        "\r\n",
        "batch_size = 512\r\n",
        "epochs = 8\r\n",
        "\r\n",
        "model_lstm = Sequential()\r\n",
        "model_lstm.add(Embedding(vocab_size,100,input_length=100))\r\n",
        "#tf.keras.layers.Flatten()\r\n",
        "model_lstm.add(SpatialDropout1D(0.3))\r\n",
        "model_lstm.add(LSTM(256, dropout = 0.3, recurrent_dropout = 0.3,activation='relu'))\r\n",
        "model_lstm.add(Dense(3, activation='softmax'))\r\n",
        "model_lstm.compile(\r\n",
        "    loss='categorical_crossentropy',\r\n",
        "    optimizer='adam',\r\n",
        "    metrics=['AUC']\r\n",
        ")\r\n",
        "\r\n",
        "history = model_lstm.fit(\r\n",
        "    X,\r\n",
        "    Y,\r\n",
        "    validation_split = 0.15,\r\n",
        "    epochs = epochs,\r\n",
        "    batch_size = 256,callbacks=EarlyStopping(monitor=\"val_loss\",\r\n",
        "                                              min_delta=0.001,\r\n",
        "                                              patience=5)\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "pred = model_lstm.predict_proba(X_test)\r\n",
        "print(pred[:5])\r\n",
        "\r\n",
        "submit = pd.DataFrame({\r\n",
        "    'id':id,\r\n",
        "    'EAP':pred[:,0],\r\n",
        "    'HPL':pred[:,1],\r\n",
        "    'MWS':pred[:,2]\r\n",
        "})\r\n",
        "submit.to_csv('/content/drive/MyDrive/spook/submissionfirst.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n",
            "Epoch 1/8\n",
            "66/66 [==============================] - 165s 2s/step - loss: 184.2829 - auc: 0.5803 - val_loss: 1.0567 - val_auc: 0.6741\n",
            "Epoch 2/8\n",
            "66/66 [==============================] - 159s 2s/step - loss: 6.2118 - auc: 0.6345 - val_loss: 1.0423 - val_auc: 0.6781\n",
            "Epoch 3/8\n",
            "66/66 [==============================] - 158s 2s/step - loss: 86.0316 - auc: 0.6649 - val_loss: 1.0211 - val_auc: 0.6950\n",
            "Epoch 4/8\n",
            "66/66 [==============================] - 159s 2s/step - loss: 1.0060 - auc: 0.7124 - val_loss: 0.9973 - val_auc: 0.7224\n",
            "Epoch 5/8\n",
            "66/66 [==============================] - 158s 2s/step - loss: 0.9717 - auc: 0.7618 - val_loss: 0.9695 - val_auc: 0.7501\n",
            "Epoch 6/8\n",
            "66/66 [==============================] - 159s 2s/step - loss: 0.9460 - auc: 0.7717 - val_loss: 1.3326 - val_auc: 0.6818\n",
            "Epoch 7/8\n",
            "66/66 [==============================] - 161s 2s/step - loss: 1.3100 - auc: 0.6372 - val_loss: 0.9956 - val_auc: 0.7302\n",
            "Epoch 8/8\n",
            "66/66 [==============================] - 161s 2s/step - loss: 0.9491 - auc: 0.7746 - val_loss: 0.9712 - val_auc: 0.7410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.43521097 0.27547234 0.2893167 ]\n",
            " [0.5939909  0.20157175 0.20443732]\n",
            " [0.48781687 0.2682728  0.2439103 ]\n",
            " [0.56647307 0.2393548  0.19417214]\n",
            " [0.45242772 0.26294956 0.28462276]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "PxzGIgqFbsCW",
        "outputId": "518030ea-5682-4eb1-fe02-d6d2cb87fc36"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('dark_background')\n",
        "from nltk.corpus import stopwords\n",
        "stop=set(stopwords.words('english'))\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "##data loading\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/spook/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/spook/test.csv')\n",
        "submission_file = pd.read_csv('/content/drive/MyDrive/spook/sample_submission.csv')\n",
        "\n",
        "id = test_df['id']\n",
        "test_df.drop(test_df[['id']],1,inplace=True)\n",
        "train_df.drop(train_df[['id']],1,inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_df['author'].replace('EAP',1,inplace=True)\n",
        "train_df['author'].replace('HPL',2,inplace=True)\n",
        "train_df['author'].replace('MWS',3,inplace=True)\n",
        "\n",
        "def remove_nums(num):\n",
        "    return ''.join(i for i in num if not i.isdigit())\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(remove_nums)\n",
        "test_df['text'] = test_df['text'].apply(remove_nums)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = [w for w in text if w not in stopwords.words('english')]\n",
        "    return words\n",
        "    \n",
        "train_df['text'] = train_df['text'].apply(remove_stopwords)\n",
        "test_df['text'] = test_df['text'].apply(remove_stopwords)\n",
        "\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(\n",
        "    num_words = max_words,\n",
        "    filters = '\"#$%&()*+-/:;<=>@[\\]^_`{|}~',\n",
        "    lower=True\n",
        ")\n",
        "\n",
        "##vectorize characters into numbers\n",
        "tokenizer.fit_on_texts(train_df['text'])\n",
        "tokenizer.fit_on_texts(test_df['text'])\n",
        "\n",
        "X = tokenizer.texts_to_sequences(train_df['text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_df['text'])\n",
        "\n",
        "X  = pad_sequences(X,maxlen=100)\n",
        "X_test = pad_sequences(X_test,maxlen=100)\n",
        "X = X.flatten()\n",
        "X_test = X_test.flatten()\n",
        "X = X.reshape(19579,-1)\n",
        "X_test = X_test.reshape(len(X_test),-1)\n",
        "Y = np.array(train_df['author'])\n",
        "Y = Y.reshape(len(Y),-1)\n",
        "print(Y[:4])\n",
        "\n",
        "\n",
        "log_clf = LogisticRegression(multi_class='multinomial',C=0.1,penalty='l2')\n",
        "\n",
        "log_clf.fit(X,Y)\n",
        "\n",
        "pred = log_clf.predict_proba(Y)\n",
        "\n",
        "submit = pd.DataFrame({\n",
        "    'id':id,\n",
        "    'EAP':pred[:,0],\n",
        "    'HPL':pred[:,1],\n",
        "    'MWS':pred[:,2]\n",
        "})\n",
        "\n",
        "submit.to_csv('/content/drive/MyDrive/spook/submissionlogit.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1]\n",
            " [2]\n",
            " [1]\n",
            " [3]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-afa5c8995287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mlog_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mlog_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1957900, 19579]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "kvQo6LmLKoTp",
        "outputId": "065ada2b-5f72-4519-8521-5bbd48562fac"
      },
      "source": [
        "a = np.array([[1,2,3,4,5],[1,2,3,4,5]])\n",
        "a = a.reshape(4,-1)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7d4564b1421d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 10 into shape (3,newaxis)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrFpf6LPi4L4",
        "outputId": "eb4c8e47-f3b3-4513-9f4f-f9cde13ae925"
      },
      "source": [
        "###implementing dl and lstm on this model\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "a = np.array([[1],[2,3,4],[5,7,8]])\r\n",
        "a = pad_sequences(a,padding='post',maxlen=2)\r\n",
        "print(a)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[[1 0]\n",
            " [3 4]\n",
            " [5 7]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}