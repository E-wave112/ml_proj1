{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1], dtype=int64), array([4342, 3271]))\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, model_selection, preprocessing\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "train_df = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\train.csv')\n",
    "test_df = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\test.csv')\n",
    "submission_file = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\sample_submission.csv')\n",
    "train_df.head(10)\n",
    "print(np.unique(train_df.target,return_counts=True))\n",
    "\n",
    "##vecorize our text\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "## let's get counts:unique words for the first 5 tweets in the data\n",
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "print(type(train_vectors))\n",
    "##by using only transform you ensure that the trained_data tokens are mapped tp the test data tokens\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text\"])\n",
    "#print(example_train_vectors)\n",
    "# print(example_train_vectors[0].todense().shape)\n",
    "# print(example_train_vectors[0].todense())\n",
    "# print(example_train_vectors[3].todense().shape)\n",
    "# print(example_train_vectors[3].todense())\n",
    "\n",
    "##generate a baseline model using svm\n",
    "param_distribution = {\n",
    "    'C':[0.01,0.05,0.1,0.5,1],\n",
    "    \n",
    "    'loss':['hinge','squared_hinge']\n",
    "}\n",
    "# rnd_search = RandomizedSearchCV(LinearSVC(random_state=10),param_distribution,scoring='f1',n_jobs=2,verbose=1)\n",
    "# rnd_search.fit(train_vectors, train_df[\"target\"])\n",
    "# print(rnd_search.best_params_)\n",
    "\n",
    "svm_clf = LinearSVC(C=0.05)\n",
    "svm_clf.fit(train_vectors, train_df[\"target\"])\n",
    "pred = svm_clf.predict(test_vectors)\n",
    "submission_file ['target'] = pred\n",
    "submission_file.to_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "##trying out the nlp classifier with tfidf\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "\n",
    "##import data neede for modelling\n",
    "train_df = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\train.csv')\n",
    "test_df = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\test.csv')\n",
    "submission_file = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\sample_submission.csv')\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors= vectorizer.fit_transform(train_df['text'])\n",
    "test_vectors= vectorizer.transform(test_df['text'])\n",
    "svm_clf = MultinomialNB()\n",
    "svm_clf.fit(train_vectors, train_df[\"target\"])\n",
    "pred = svm_clf.predict(test_vectors)\n",
    "print(pred[:5])\n",
    "submission_file ['target'] = pred\n",
    "submission_file.to_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\submissiontf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1f8c185d0bb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "##implementing an nlp model with rnn and lstm\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stop=set(stopwords.words('english'))\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import string\n",
    "\n",
    "train_df = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\train.csv')\n",
    "test_df = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\test.csv')\n",
    "submission_file = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\sample_submission.csv')\n",
    "\n",
    "def remove_nick(text):\n",
    "    return re.sub(r\"\\@\\S+\", \"\",text)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x:remove_nick(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x:remove_nick(x))\n",
    "\n",
    "def remove_nums(num):\n",
    "    return ''.join(i for i in num if not i.isdigit())\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x:remove_nums(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x:remove_nums(x))\n",
    "\n",
    "def remove_url(y):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(r'',\"\", y)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x:remove_url(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x:remove_url(x))\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x:remove_punctuations(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x:remove_punctuations(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x:remove_stopwords(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x:remove_stopwords(x))\n",
    "\n",
    "train_df = train_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "test_df = test_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorizer.fit_transform(train_df['text'])\n",
    "vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "##implementing an nlp model with rnn and lstm\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words('english'))\n",
    "import string\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_df = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\train.csv')\n",
    "test_df = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\test.csv')\n",
    "submission_file = pd.read_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\sample_submission.csv')\n",
    "\n",
    "\n",
    "def remove_nick(text):\n",
    "    return re.sub(r\"\\@\\S+\", \"\",text)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x:remove_nick(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x:remove_nick(x))\n",
    "\n",
    "def remove_nums(num):\n",
    "    return ''.join(i for i in num if not i.isdigit())\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x:remove_nums(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x:remove_nums(x))\n",
    "\n",
    "def remove_url(y):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(r'',\"\", y)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x:remove_url(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x:remove_url(x))\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(lambda x:remove_punctuations(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x:remove_punctuations(x))\n",
    "\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "\n",
    "#     words = [w for w in text if w not in stopwords.words('english')]\n",
    "#     return words[i for i in range(len(words))]\n",
    "\n",
    "# train_df['text'] = train_df['text'].apply(lambda x:remove_stopwords(x))\n",
    "# test_df['text'] = test_df['text'].apply(lambda x:remove_stopwords(x))\n",
    "\n",
    "train_df = train_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "test_df = test_df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_df['text']).toarray()\n",
    "test_vectors = vectorizer.transform(test_df['text']).toarray()\n",
    "\n",
    "#modelling\n",
    "\n",
    "gnb_clf = GaussianNB()\n",
    "gnb_clf.fit(train_vectors,train_df['target'])\n",
    "\n",
    "pred = gnb_clf.predict(test_vectors)\n",
    "\n",
    "\n",
    "submission_file['target'] = pred\n",
    "submission_file.to_csv('C:\\\\Users\\\\E-wave\\\\Documents\\\\tnlp\\\\subnb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n"
     ]
    }
   ],
   "source": [
    "print(chr(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
